{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed8b9eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0041e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # Restrict TensorFlow to only allocate 1GB of memory on the first GPU\n",
    "  try:\n",
    "    tf.config.set_logical_device_configuration(\n",
    "        gpus[0],\n",
    "        [tf.config.LogicalDeviceConfiguration(memory_limit=6024)])\n",
    "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Virtual devices must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3478001f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d121ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p \"kaggle\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae79686",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "shutil.copy('kaggle.json','kaggle/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb703aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Specify the file path and the desired permissions\n",
    "file_path = 'kaggle/kaggle.json'\n",
    "permissions = 600  # Example: Read and write permissions for the owner only\n",
    "\n",
    "# Change the file permissions using the os module\n",
    "os.chmod(file_path, permissions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a17a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "! kaggle datasets download nikhilroxtomar/person-segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ec69d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "! kaggle datasets download -d tapakah68/supervisely-filtered-segmentation-person-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d28f5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "! kaggle datasets download -d kapitanov/easyportrait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2bf18e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from glob import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Conv2D, BatchNormalization, Activation, MaxPool2D, Conv2DTranspose, Concatenate, Input\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, CSVLogger, EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import urllib\n",
    "import IPython"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1061e56f",
   "metadata": {},
   "source": [
    "img = cv2.imread(\"E:\\College\\Term2\\AI ML Lab\\Backgrougd Removal\\Background Dataset\\masks\\cycling-bike-trail-sport-159237.png\")\n",
    "img1 = cv2.imread(\"E:\\College\\Term2\\AI ML Lab\\Backgrougd Removal\\Background Dataset\\masks\\ds1_bow-tie-businessman-fashion-man.png\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "295323a9",
   "metadata": {},
   "source": [
    "a = np.array(img)\n",
    "b = np.array(img1)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d6ccc90f",
   "metadata": {},
   "source": [
    "a *= 255"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fef0c740",
   "metadata": {},
   "source": [
    "np.unique(a)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7a2a30d8",
   "metadata": {},
   "source": [
    "cv2.imwrite(\"cycling-bike-trail-sport-159237.png\", a)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "005a6cb1",
   "metadata": {},
   "source": [
    "np.unique(a)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f34b4756",
   "metadata": {},
   "source": [
    "def convert_to_mask(image_path):\n",
    "    # Load the image\n",
    "    image = cv2.imread(image_path)\n",
    "\n",
    "    # Convert the image to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Threshold the grayscale image to create a binary mask\n",
    "    _, binary_mask = cv2.threshold(gray, 1, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    # Save the resulting mask\n",
    "    output_path = \"mask.png\"\n",
    "    cv2.imwrite(output_path, binary_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83eb6396",
   "metadata": {},
   "source": [
    "convert_to_mask('ds1_pexels-photo-175697.png')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "11be899f",
   "metadata": {},
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load the mask image with values 0 and 1\n",
    "mask_image_01 = cv2.imread('mask_image_01.png', 0)  # Assuming grayscale image\n",
    "\n",
    "# Multiply the mask image by 255 to convert values from 0 and 1 to 0 and 255\n",
    "mask_image_01 = mask_image_01 * 255\n",
    "\n",
    "# Load the mask image with values 0 and 255\n",
    "mask_image_0255 = cv2.imread('mask_image_0255.png', 0)  # Assuming grayscale image\n",
    "\n",
    "# Divide the mask image by 255 to convert values from 0 and 255 to 0 and 1\n",
    "mask_image_0255 = np.divide(mask_image_0255, 255)\n",
    "\n",
    "# Display or further process the modified mask images\n",
    "cv2.imshow('Modified Mask Image 01', mask_image_01)\n",
    "cv2.imshow('Modified Mask Image 0255', mask_image_0255)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d31f83f7",
   "metadata": {},
   "source": [
    "def convert_mask(paths):\n",
    "    for path in paths:\n",
    "        img  = cv2.imread(f'Backgrougd Removal/EasyPortrait/annotations/train/{path}')\n",
    "        img[img > 0] = 255\n",
    "        nm = path.split(\"/\")[-1]\n",
    "        cv2.imwrite(f'Backgrougd Removal/EasyPortrait/annotations/Train 1/{path}', img)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "af8c5065",
   "metadata": {},
   "source": [
    "paths = os.listdir('Backgrougd Removal/EasyPortrait/annotations/train')\n",
    "paths[:5]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "dc046054",
   "metadata": {},
   "source": [
    "convert_mask(paths)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "71e2ad48",
   "metadata": {},
   "source": [
    "paths = os.listdir('Backgrougd Removal/Background Dataset/masks')\n",
    "paths"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9fa5bfa7",
   "metadata": {},
   "source": [
    "convert_mask(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1924de32",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Model Bulding Code\"\"\"\n",
    "\n",
    "def conv_block(inputs, num_filters):\n",
    "    x = Conv2D(num_filters, 3, padding='same')(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "\n",
    "    x = Conv2D(num_filters, 3, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "def encoder_block(inputs, num_filters):\n",
    "    x = conv_block(inputs, num_filters)\n",
    "    p = MaxPool2D((2, 2))(x)\n",
    "    return x, p\n",
    "\n",
    "def decoder_block(inputs, skip_features, num_filters):\n",
    "    x = Conv2DTranspose(num_filters, (2, 2), strides=2, padding='same')(inputs)\n",
    "    x = Concatenate()([x, skip_features])\n",
    "    x = conv_block(x, num_filters)\n",
    "    return x\n",
    "\n",
    "def build_unet(input_shape):\n",
    "    inputs = Input(input_shape)\n",
    "\n",
    "    \"\"\" Encoder \"\"\"\n",
    "    s1, p1 = encoder_block(inputs, 64)\n",
    "    s2, p2 = encoder_block(p1, 128)\n",
    "    s3, p3 = encoder_block(p2, 256)\n",
    "    s4, p4 = encoder_block(p3, 512)\n",
    "    s5, p5 = encoder_block(p4, 1024)\n",
    "    b1 = conv_block(p5, 2048)\n",
    "\n",
    "    \"\"\" Decoder \"\"\"\n",
    "    d1 = decoder_block(b1, s5, 1024)\n",
    "    d2 = decoder_block(d1, s4, 512)\n",
    "    d3 = decoder_block(d2, s3, 256)\n",
    "    d4 = decoder_block(d3, s2, 128)\n",
    "    d5 = decoder_block(d4, s1, 64)\n",
    "\n",
    "    \"\"\" Output \"\"\"\n",
    "    outputs = Conv2D(1, (1, 1), padding=\"same\", activation=\"sigmoid\")(d5)\n",
    "    return Model(inputs, outputs, name=\"U-Net\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5c6da16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Dataset preparation Code\"\"\"\n",
    "\n",
    "def augment(image, mask, s):\n",
    "    flip = layers.RandomFlip(\"horizontal_and_vertical\", seed=s) # or \"horizontal\", \"vertical\"\n",
    "    rotate = tf.keras.layers.RandomRotation(0.3, seed=s)\n",
    "    zoom = tf.keras.layers.RandomZoom(0.3,seed=s)\n",
    "    lst = [flip, rotate, zoom]\n",
    "    for i in range(len(lst)):\n",
    "        image = lst[i](image)\n",
    "        mask = lst[i](mask)\n",
    "        \n",
    "    return image, mask\n",
    "\n",
    "def load_data(dataset_path):\n",
    "    images = sorted(glob(os.path.join(dataset_path, \"images/*\")))\n",
    "    masks = sorted(glob(os.path.join(dataset_path, \"masks_/*\")))\n",
    "#     for i in range(2):\n",
    "#         print(images[i])\n",
    "#         print(masks[i])\n",
    "    train_x, test_x = train_test_split(images, test_size=0.1, random_state=42)\n",
    "    train_y, test_y = train_test_split(masks, test_size=0.1, random_state=42)\n",
    "    return (train_x, train_y), (test_x, test_y)\n",
    "\n",
    "def read_image(path):\n",
    "    x = cv2.imread(path, cv2.IMREAD_COLOR)\n",
    "    x = cv2.resize(x, (256, 256))\n",
    "    x = x/255.0\n",
    "    x = x.astype(np.float32)\n",
    "    # (256, 256, 3)\n",
    "    return x\n",
    "\n",
    "def read_mask(path):\n",
    "    x = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "    x = cv2.resize(x, (256, 256))\n",
    "    x = x/255.0\n",
    "    x = x.astype(np.float32)\n",
    "    x = np.expand_dims(x, axis=-1)\n",
    "    return x\n",
    "\n",
    "def preprocess(image_path, mask_path):\n",
    "    def f(image_path, mask_path):\n",
    "        image_path = image_path.decode()\n",
    "        mask_path = mask_path.decode()\n",
    "        x = read_image(image_path)\n",
    "        y = read_mask(mask_path)\n",
    "        return x, y\n",
    "    image, mask = tf.numpy_function(f, [image_path, mask_path], [tf.float32, tf.float32])\n",
    "    seed = np.random.randint(500)\n",
    "    image, mask = augment(image, mask, seed)    \n",
    "    image.set_shape([256, 256, 3])\n",
    "    mask.set_shape([256, 256, 1])\n",
    "    return image, mask\n",
    "\n",
    "def tf_dataset(images, masks, batch=8):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((images, masks))\n",
    "    dataset = dataset.shuffle(buffer_size=30000)\n",
    "    dataset = dataset.map(preprocess)\n",
    "    dataset = dataset.batch(batch)\n",
    "    dataset = dataset.prefetch(2)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "732c68b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Hyperparameters \"\"\"\n",
    "dataset_path = \"Backgrougd Removal/Background Dataset/\"\n",
    "input_shape = (256, 256, 3)\n",
    "batch_size = 8\n",
    "epochs = 60\n",
    "lr = 1e-3\n",
    "model_path = \"./RamBG Data/Models/unet_july_15.h5\"\n",
    "csv_path = \"./RamBG Data/data_july_15.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8022fb95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Background Dataset',\n",
       " 'Background Dataset - Shortcut.lnk',\n",
       " 'EasyPortrait',\n",
       " 'easyportrait.zip',\n",
       " 'masks',\n",
       " 'people_segmentation',\n",
       " 'person-segmentation.zip',\n",
       " 'supervisely-filtered-segmentation-person-dataset.zip',\n",
       " 'supervisely_person_clean_2667_img',\n",
       " 'Test']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.listdir(\"Backgrougd Removal/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6a469385",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Loading the dataset \"\"\"\n",
    "(train_x, train_y), (test_x, test_y) = load_data(dataset_path)\n",
    "\n",
    "train_dataset = tf_dataset(train_x, train_y, batch=batch_size)\n",
    "valid_dataset = tf_dataset(test_x, test_y, batch=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d8900b95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "330"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9c6a9bb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TakeDataset element_spec=(TensorSpec(shape=(None, 256, 256, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 256, 256, 1), dtype=tf.float32, name=None))>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8aa43d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.skip(700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7b7a89a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2264"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a9302655",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Model \"\"\"\n",
    "model = build_unet(input_shape)\n",
    "model.compile(\n",
    "    loss=\"binary_crossentropy\",\n",
    "    optimizer=tf.keras.optimizers.Adam(lr),\n",
    "    metrics=[\n",
    "        tf.keras.metrics.MeanIoU(num_classes=2),\n",
    "        tf.keras.metrics.Recall(),\n",
    "        tf.keras.metrics.Precision()\n",
    "    ]\n",
    ")\n",
    "\n",
    "callbacks = [\n",
    "    ModelCheckpoint(model_path, monitor=\"val_loss\", verbose=1, save_best_only=False),\n",
    "    ReduceLROnPlateau(monitor=\"val_loss\", patience=4, factor=0.1, verbose=1),\n",
    "    CSVLogger(csv_path),\n",
    "    EarlyStopping(monitor=\"val_loss\", patience=10, restore_best_weights=False)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d54600cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_steps = len(train_x)//batch_size - 700\n",
    "if len(train_x) % batch_size != 0:\n",
    "    train_steps += 1\n",
    "\n",
    "valid_steps = len(test_x)//batch_size\n",
    "if len(test_x) % batch_size != 0:\n",
    "    valid_steps += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3872f50a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "330"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "aca04cc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2264"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "58250d40",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "2264/2264 [==============================] - ETA: 0s - loss: 0.2784 - mean_io_u_1: 0.2883 - recall_1: 0.8522 - precision_1: 0.8619\n",
      "Epoch 1: saving model to ./RamBG Data/Models\\unet_july_15.h5\n",
      "2264/2264 [==============================] - 1399s 506ms/step - loss: 0.2784 - mean_io_u_1: 0.2883 - recall_1: 0.8522 - precision_1: 0.8619 - val_loss: 0.2294 - val_mean_io_u_1: 0.2849 - val_recall_1: 0.8559 - val_precision_1: 0.9123 - lr: 0.0010\n",
      "Epoch 2/60\n",
      "2264/2264 [==============================] - ETA: 0s - loss: 0.1570 - mean_io_u_1: 0.2886 - recall_1: 0.9258 - precision_1: 0.9275\n",
      "Epoch 2: saving model to ./RamBG Data/Models\\unet_july_15.h5\n",
      "2264/2264 [==============================] - 1291s 468ms/step - loss: 0.1570 - mean_io_u_1: 0.2886 - recall_1: 0.9258 - precision_1: 0.9275 - val_loss: 0.1473 - val_mean_io_u_1: 0.2858 - val_recall_1: 0.9513 - val_precision_1: 0.9242 - lr: 0.0010\n",
      "Epoch 3/60\n",
      "2264/2264 [==============================] - ETA: 0s - loss: 0.1171 - mean_io_u_1: 0.2886 - recall_1: 0.9470 - precision_1: 0.9475\n",
      "Epoch 3: saving model to ./RamBG Data/Models\\unet_july_15.h5\n",
      "2264/2264 [==============================] - 1366s 501ms/step - loss: 0.1171 - mean_io_u_1: 0.2886 - recall_1: 0.9470 - precision_1: 0.9475 - val_loss: 0.1356 - val_mean_io_u_1: 0.2903 - val_recall_1: 0.9710 - val_precision_1: 0.9192 - lr: 0.0010\n",
      "Epoch 4/60\n",
      "2264/2264 [==============================] - ETA: 0s - loss: 0.0981 - mean_io_u_1: 0.2889 - recall_1: 0.9560 - precision_1: 0.9563\n",
      "Epoch 4: saving model to ./RamBG Data/Models\\unet_july_15.h5\n",
      "2264/2264 [==============================] - 1305s 474ms/step - loss: 0.0981 - mean_io_u_1: 0.2889 - recall_1: 0.9560 - precision_1: 0.9563 - val_loss: 0.1062 - val_mean_io_u_1: 0.2849 - val_recall_1: 0.9413 - val_precision_1: 0.9635 - lr: 0.0010\n",
      "Epoch 5/60\n",
      "2264/2264 [==============================] - ETA: 0s - loss: 0.0851 - mean_io_u_1: 0.2893 - recall_1: 0.9620 - precision_1: 0.9626\n",
      "Epoch 5: saving model to ./RamBG Data/Models\\unet_july_15.h5\n",
      "2264/2264 [==============================] - 1411s 521ms/step - loss: 0.0851 - mean_io_u_1: 0.2893 - recall_1: 0.9620 - precision_1: 0.9626 - val_loss: 0.0957 - val_mean_io_u_1: 0.2849 - val_recall_1: 0.9645 - val_precision_1: 0.9517 - lr: 0.0010\n",
      "Epoch 6/60\n",
      "2264/2264 [==============================] - ETA: 0s - loss: 0.0760 - mean_io_u_1: 0.2905 - recall_1: 0.9664 - precision_1: 0.9659\n",
      "Epoch 6: saving model to ./RamBG Data/Models\\unet_july_15.h5\n",
      "2264/2264 [==============================] - 1293s 468ms/step - loss: 0.0760 - mean_io_u_1: 0.2905 - recall_1: 0.9664 - precision_1: 0.9659 - val_loss: 0.0853 - val_mean_io_u_1: 0.3011 - val_recall_1: 0.9751 - val_precision_1: 0.9532 - lr: 0.0010\n",
      "Epoch 7/60\n",
      "2264/2264 [==============================] - ETA: 0s - loss: 0.0683 - mean_io_u_1: 0.2897 - recall_1: 0.9697 - precision_1: 0.9699\n",
      "Epoch 7: saving model to ./RamBG Data/Models\\unet_july_15.h5\n",
      "2264/2264 [==============================] - 1372s 504ms/step - loss: 0.0683 - mean_io_u_1: 0.2897 - recall_1: 0.9697 - precision_1: 0.9699 - val_loss: 0.0772 - val_mean_io_u_1: 0.2849 - val_recall_1: 0.9640 - val_precision_1: 0.9712 - lr: 0.0010\n",
      "Epoch 8/60\n",
      "2264/2264 [==============================] - ETA: 0s - loss: 0.0598 - mean_io_u_1: 0.2912 - recall_1: 0.9740 - precision_1: 0.9731\n",
      "Epoch 8: saving model to ./RamBG Data/Models\\unet_july_15.h5\n",
      "2264/2264 [==============================] - 1401s 515ms/step - loss: 0.0598 - mean_io_u_1: 0.2912 - recall_1: 0.9740 - precision_1: 0.9731 - val_loss: 0.0726 - val_mean_io_u_1: 0.2852 - val_recall_1: 0.9697 - val_precision_1: 0.9705 - lr: 0.0010\n",
      "Epoch 9/60\n",
      "2264/2264 [==============================] - ETA: 0s - loss: 0.0558 - mean_io_u_1: 0.2901 - recall_1: 0.9756 - precision_1: 0.9753\n",
      "Epoch 9: saving model to ./RamBG Data/Models\\unet_july_15.h5\n",
      "2264/2264 [==============================] - 1333s 488ms/step - loss: 0.0558 - mean_io_u_1: 0.2901 - recall_1: 0.9756 - precision_1: 0.9753 - val_loss: 0.0727 - val_mean_io_u_1: 0.2849 - val_recall_1: 0.9620 - val_precision_1: 0.9753 - lr: 0.0010\n",
      "Epoch 10/60\n",
      "2264/2264 [==============================] - ETA: 0s - loss: 0.0509 - mean_io_u_1: 0.2915 - recall_1: 0.9777 - precision_1: 0.9776\n",
      "Epoch 10: saving model to ./RamBG Data/Models\\unet_july_15.h5\n",
      "2264/2264 [==============================] - 1298s 470ms/step - loss: 0.0509 - mean_io_u_1: 0.2915 - recall_1: 0.9777 - precision_1: 0.9776 - val_loss: 0.0922 - val_mean_io_u_1: 0.2849 - val_recall_1: 0.9453 - val_precision_1: 0.9844 - lr: 0.0010\n",
      "Epoch 11/60\n",
      "2264/2264 [==============================] - ETA: 0s - loss: 0.0473 - mean_io_u_1: 0.2910 - recall_1: 0.9795 - precision_1: 0.9791\n",
      "Epoch 11: saving model to ./RamBG Data/Models\\unet_july_15.h5\n",
      "2264/2264 [==============================] - 1313s 477ms/step - loss: 0.0473 - mean_io_u_1: 0.2910 - recall_1: 0.9795 - precision_1: 0.9791 - val_loss: 0.0644 - val_mean_io_u_1: 0.2849 - val_recall_1: 0.9747 - val_precision_1: 0.9709 - lr: 0.0010\n",
      "Epoch 12/60\n",
      "2264/2264 [==============================] - ETA: 0s - loss: 0.0438 - mean_io_u_1: 0.2911 - recall_1: 0.9808 - precision_1: 0.9806\n",
      "Epoch 12: saving model to ./RamBG Data/Models\\unet_july_15.h5\n",
      "2264/2264 [==============================] - 1289s 466ms/step - loss: 0.0438 - mean_io_u_1: 0.2911 - recall_1: 0.9808 - precision_1: 0.9806 - val_loss: 0.0710 - val_mean_io_u_1: 0.2849 - val_recall_1: 0.9708 - val_precision_1: 0.9695 - lr: 0.0010\n",
      "Epoch 13/60\n",
      "2264/2264 [==============================] - ETA: 0s - loss: 0.0392 - mean_io_u_1: 0.2916 - recall_1: 0.9827 - precision_1: 0.9827\n",
      "Epoch 13: saving model to ./RamBG Data/Models\\unet_july_15.h5\n",
      "2264/2264 [==============================] - 1284s 465ms/step - loss: 0.0392 - mean_io_u_1: 0.2916 - recall_1: 0.9827 - precision_1: 0.9827 - val_loss: 0.0677 - val_mean_io_u_1: 0.2849 - val_recall_1: 0.9643 - val_precision_1: 0.9793 - lr: 0.0010\n",
      "Epoch 14/60\n",
      "2264/2264 [==============================] - ETA: 0s - loss: 0.0377 - mean_io_u_1: 0.2906 - recall_1: 0.9834 - precision_1: 0.9834\n",
      "Epoch 14: saving model to ./RamBG Data/Models\\unet_july_15.h5\n",
      "2264/2264 [==============================] - 1383s 508ms/step - loss: 0.0377 - mean_io_u_1: 0.2906 - recall_1: 0.9834 - precision_1: 0.9834 - val_loss: 0.0587 - val_mean_io_u_1: 0.2849 - val_recall_1: 0.9765 - val_precision_1: 0.9759 - lr: 0.0010\n",
      "Epoch 15/60\n",
      "2264/2264 [==============================] - ETA: 0s - loss: 0.0345 - mean_io_u_1: 0.2913 - recall_1: 0.9847 - precision_1: 0.9850\n",
      "Epoch 15: saving model to ./RamBG Data/Models\\unet_july_15.h5\n",
      "2264/2264 [==============================] - 1563s 589ms/step - loss: 0.0345 - mean_io_u_1: 0.2913 - recall_1: 0.9847 - precision_1: 0.9850 - val_loss: 0.0577 - val_mean_io_u_1: 0.2856 - val_recall_1: 0.9783 - val_precision_1: 0.9770 - lr: 0.0010\n",
      "Epoch 16/60\n",
      "2264/2264 [==============================] - ETA: 0s - loss: 0.0327 - mean_io_u_1: 0.2941 - recall_1: 0.9853 - precision_1: 0.9859\n",
      "Epoch 16: saving model to ./RamBG Data/Models\\unet_july_15.h5\n",
      "2264/2264 [==============================] - 1290s 468ms/step - loss: 0.0327 - mean_io_u_1: 0.2941 - recall_1: 0.9853 - precision_1: 0.9859 - val_loss: 0.0572 - val_mean_io_u_1: 0.2849 - val_recall_1: 0.9764 - val_precision_1: 0.9794 - lr: 0.0010\n",
      "Epoch 17/60\n",
      "2264/2264 [==============================] - ETA: 0s - loss: 0.0302 - mean_io_u_1: 0.2952 - recall_1: 0.9862 - precision_1: 0.9870\n",
      "Epoch 17: saving model to ./RamBG Data/Models\\unet_july_15.h5\n",
      "2264/2264 [==============================] - 1342s 490ms/step - loss: 0.0302 - mean_io_u_1: 0.2952 - recall_1: 0.9862 - precision_1: 0.9870 - val_loss: 0.0591 - val_mean_io_u_1: 0.2850 - val_recall_1: 0.9715 - val_precision_1: 0.9830 - lr: 0.0010\n",
      "Epoch 18/60\n",
      "2264/2264 [==============================] - ETA: 0s - loss: 0.0293 - mean_io_u_1: 0.2949 - recall_1: 0.9865 - precision_1: 0.9876\n",
      "Epoch 18: saving model to ./RamBG Data/Models\\unet_july_15.h5\n",
      "2264/2264 [==============================] - 1422s 525ms/step - loss: 0.0293 - mean_io_u_1: 0.2949 - recall_1: 0.9865 - precision_1: 0.9876 - val_loss: 0.0602 - val_mean_io_u_1: 0.2849 - val_recall_1: 0.9745 - val_precision_1: 0.9793 - lr: 0.0010\n",
      "Epoch 19/60\n",
      "2264/2264 [==============================] - ETA: 0s - loss: 0.0269 - mean_io_u_1: 0.2956 - recall_1: 0.9875 - precision_1: 0.9887\n",
      "Epoch 19: saving model to ./RamBG Data/Models\\unet_july_15.h5\n",
      "2264/2264 [==============================] - 1291s 468ms/step - loss: 0.0269 - mean_io_u_1: 0.2956 - recall_1: 0.9875 - precision_1: 0.9887 - val_loss: 0.0618 - val_mean_io_u_1: 0.2849 - val_recall_1: 0.9697 - val_precision_1: 0.9841 - lr: 0.0010\n",
      "Epoch 20/60\n",
      "2264/2264 [==============================] - ETA: 0s - loss: 0.0259 - mean_io_u_1: 0.2987 - recall_1: 0.9877 - precision_1: 0.9893\n",
      "Epoch 20: saving model to ./RamBG Data/Models\\unet_july_15.h5\n",
      "\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "2264/2264 [==============================] - 1301s 471ms/step - loss: 0.0259 - mean_io_u_1: 0.2987 - recall_1: 0.9877 - precision_1: 0.9893 - val_loss: 0.0606 - val_mean_io_u_1: 0.2853 - val_recall_1: 0.9780 - val_precision_1: 0.9779 - lr: 0.0010\n",
      "Epoch 21/60\n",
      "2264/2264 [==============================] - ETA: 0s - loss: 0.0220 - mean_io_u_1: 0.2969 - recall_1: 0.9894 - precision_1: 0.9909\n",
      "Epoch 21: saving model to ./RamBG Data/Models\\unet_july_15.h5\n",
      "2264/2264 [==============================] - 1311s 476ms/step - loss: 0.0220 - mean_io_u_1: 0.2969 - recall_1: 0.9894 - precision_1: 0.9909 - val_loss: 0.0544 - val_mean_io_u_1: 0.2850 - val_recall_1: 0.9790 - val_precision_1: 0.9829 - lr: 1.0000e-04\n",
      "Epoch 22/60\n",
      "2264/2264 [==============================] - ETA: 0s - loss: 0.0200 - mean_io_u_1: 0.3029 - recall_1: 0.9901 - precision_1: 0.9918\n",
      "Epoch 22: saving model to ./RamBG Data/Models\\unet_july_15.h5\n",
      "2264/2264 [==============================] - 1357s 497ms/step - loss: 0.0200 - mean_io_u_1: 0.3029 - recall_1: 0.9901 - precision_1: 0.9918 - val_loss: 0.0547 - val_mean_io_u_1: 0.2858 - val_recall_1: 0.9800 - val_precision_1: 0.9830 - lr: 1.0000e-04\n",
      "Epoch 23/60\n",
      "2264/2264 [==============================] - ETA: 0s - loss: 0.0193 - mean_io_u_1: 0.3072 - recall_1: 0.9905 - precision_1: 0.9922\n",
      "Epoch 23: saving model to ./RamBG Data/Models\\unet_july_15.h5\n",
      "2264/2264 [==============================] - 1636s 621ms/step - loss: 0.0193 - mean_io_u_1: 0.3072 - recall_1: 0.9905 - precision_1: 0.9922 - val_loss: 0.0546 - val_mean_io_u_1: 0.2873 - val_recall_1: 0.9801 - val_precision_1: 0.9832 - lr: 1.0000e-04\n",
      "Epoch 24/60\n",
      "2264/2264 [==============================] - ETA: 0s - loss: 0.0186 - mean_io_u_1: 0.3119 - recall_1: 0.9907 - precision_1: 0.9925\n",
      "Epoch 24: saving model to ./RamBG Data/Models\\unet_july_15.h5\n",
      "2264/2264 [==============================] - 1289s 467ms/step - loss: 0.0186 - mean_io_u_1: 0.3119 - recall_1: 0.9907 - precision_1: 0.9925 - val_loss: 0.0561 - val_mean_io_u_1: 0.2877 - val_recall_1: 0.9797 - val_precision_1: 0.9838 - lr: 1.0000e-04\n",
      "Epoch 25/60\n",
      "2264/2264 [==============================] - ETA: 0s - loss: 0.0181 - mean_io_u_1: 0.3177 - recall_1: 0.9909 - precision_1: 0.9927\n",
      "Epoch 25: saving model to ./RamBG Data/Models\\unet_july_15.h5\n",
      "\n",
      "Epoch 25: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "2264/2264 [==============================] - 1312s 476ms/step - loss: 0.0181 - mean_io_u_1: 0.3177 - recall_1: 0.9909 - precision_1: 0.9927 - val_loss: 0.0574 - val_mean_io_u_1: 0.2926 - val_recall_1: 0.9805 - val_precision_1: 0.9831 - lr: 1.0000e-04\n",
      "Epoch 26/60\n",
      "2264/2264 [==============================] - ETA: 0s - loss: 0.0175 - mean_io_u_1: 0.3184 - recall_1: 0.9911 - precision_1: 0.9931\n",
      "Epoch 26: saving model to ./RamBG Data/Models\\unet_july_15.h5\n",
      "2264/2264 [==============================] - 1305s 474ms/step - loss: 0.0175 - mean_io_u_1: 0.3184 - recall_1: 0.9911 - precision_1: 0.9931 - val_loss: 0.0574 - val_mean_io_u_1: 0.2911 - val_recall_1: 0.9801 - val_precision_1: 0.9837 - lr: 1.0000e-05\n",
      "Epoch 27/60\n",
      "2264/2264 [==============================] - ETA: 0s - loss: 0.0175 - mean_io_u_1: 0.3194 - recall_1: 0.9912 - precision_1: 0.9930\n",
      "Epoch 27: saving model to ./RamBG Data/Models\\unet_july_15.h5\n",
      "2264/2264 [==============================] - 1446s 534ms/step - loss: 0.0175 - mean_io_u_1: 0.3194 - recall_1: 0.9912 - precision_1: 0.9930 - val_loss: 0.0571 - val_mean_io_u_1: 0.2898 - val_recall_1: 0.9805 - val_precision_1: 0.9835 - lr: 1.0000e-05\n",
      "Epoch 28/60\n",
      "2264/2264 [==============================] - ETA: 0s - loss: 0.0174 - mean_io_u_1: 0.3215 - recall_1: 0.9912 - precision_1: 0.9930\n",
      "Epoch 28: saving model to ./RamBG Data/Models\\unet_july_15.h5\n",
      "2264/2264 [==============================] - 1298s 468ms/step - loss: 0.0174 - mean_io_u_1: 0.3215 - recall_1: 0.9912 - precision_1: 0.9930 - val_loss: 0.0578 - val_mean_io_u_1: 0.2918 - val_recall_1: 0.9800 - val_precision_1: 0.9839 - lr: 1.0000e-05\n",
      "Epoch 29/60\n",
      "2264/2264 [==============================] - ETA: 0s - loss: 0.0173 - mean_io_u_1: 0.3217 - recall_1: 0.9912 - precision_1: 0.9931\n",
      "Epoch 29: saving model to ./RamBG Data/Models\\unet_july_15.h5\n",
      "\n",
      "Epoch 29: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "2264/2264 [==============================] - 1408s 518ms/step - loss: 0.0173 - mean_io_u_1: 0.3217 - recall_1: 0.9912 - precision_1: 0.9931 - val_loss: 0.0578 - val_mean_io_u_1: 0.2909 - val_recall_1: 0.9803 - val_precision_1: 0.9836 - lr: 1.0000e-05\n",
      "Epoch 30/60\n",
      "2264/2264 [==============================] - ETA: 0s - loss: 0.0172 - mean_io_u_1: 0.3220 - recall_1: 0.9912 - precision_1: 0.9931\n",
      "Epoch 30: saving model to ./RamBG Data/Models\\unet_july_15.h5\n",
      "2264/2264 [==============================] - 1311s 471ms/step - loss: 0.0172 - mean_io_u_1: 0.3220 - recall_1: 0.9912 - precision_1: 0.9931 - val_loss: 0.0582 - val_mean_io_u_1: 0.2913 - val_recall_1: 0.9803 - val_precision_1: 0.9837 - lr: 1.0000e-06\n",
      "Epoch 31/60\n",
      "2264/2264 [==============================] - ETA: 0s - loss: 0.0172 - mean_io_u_1: 0.3211 - recall_1: 0.9913 - precision_1: 0.9931\n",
      "Epoch 31: saving model to ./RamBG Data/Models\\unet_july_15.h5\n",
      "2264/2264 [==============================] - 1360s 499ms/step - loss: 0.0172 - mean_io_u_1: 0.3211 - recall_1: 0.9913 - precision_1: 0.9931 - val_loss: 0.0583 - val_mean_io_u_1: 0.2924 - val_recall_1: 0.9805 - val_precision_1: 0.9835 - lr: 1.0000e-06\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1babc81f400>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=valid_dataset,\n",
    "    epochs = epochs,\n",
    "    steps_per_epoch=train_steps,\n",
    "    validation_steps=valid_steps,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9bef1a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(\"RamBG Data/Models/unet_july_15.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61d22587",
   "metadata": {},
   "outputs": [],
   "source": [
    "A= cv2.imread(\"E:\\College\\Term2\\AI ML Lab\\Backgrougd Removal\\Background Dataset\\download (1).jpeg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5846df0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_(s1):\n",
    "    return s1.replace('\\\\','/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ff302aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'E:/Photos/Archive/E40DFFDF-2B49-478E-BC1C-DC4CF4A59CE2.JPG'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_(\"E:\\Photos\\Archive\\E40DFFDF-2B49-478E-BC1C-DC4CF4A59CE2.JPG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "672fdc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images = [\n",
    "#     convert_(r\"E:\\College\\Term2\\AI ML Lab\\Backgrougd Removal\\Background Dataset\\images\\ds1_family-toddler-hapy-happy-160688.png\")\n",
    "#     convert_(\"E:\\College\\Term2\\AI ML Lab\\Backgrougd Removal\\Background Dataset\\Test1.jpg\"),\n",
    "#     convert_(\"E:\\College\\Term2\\AI ML Lab\\Backgrougd Removal\\Background Dataset\\Test2.jpg\"),\n",
    "#     convert_(\"E:\\College\\Term2\\AI ML Lab\\Backgrougd Removal\\Background Dataset\\Test3.jpg\"),\n",
    "#     convert_(\"E:\\College\\Term2\\AI ML Lab\\Backgrougd Removal\\Background Dataset\\Test4.jpg\")\n",
    "    convert_(\"E:\\College\\Term2\\AI ML Lab\\Backgrougd Removal\\Background Dataset\\WhatsApp Image_V.jpg\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "966525f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['E:/College/Term2/AI ML Lab/Backgrougd Removal/Background Dataset/WhatsApp Image_V.jpg']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f9ec2260",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  2.32it/s]\n"
     ]
    }
   ],
   "source": [
    "for path in tqdm(test_images, total=len(test_images)):\n",
    "    # req = path\n",
    "    # imgarr = np.asarray(bytearray(req.read()), dtype=np.uint8)\n",
    "\n",
    "    # x = cv2.imdecode(imgarr, -1)\n",
    "\n",
    "    x = cv2.imread(path, cv2.IMREAD_COLOR)\n",
    "    original_image = x\n",
    "#     print(x)\n",
    "    h, w, _ = x.shape\n",
    "\n",
    "    x = cv2.resize(x, (256, 256))\n",
    "    x = x/255.0\n",
    "    x = x.astype(np.float32)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "\n",
    "    pred_mask = model.predict(x)[0]\n",
    "    pred_mask = cv2.resize(pred_mask, (w, h))\n",
    "    pred_mask = np.expand_dims(pred_mask, axis=-1)\n",
    "    pred_mask = pred_mask > 0.5\n",
    "\n",
    "    background_mask = np.abs(1- pred_mask)\n",
    "\n",
    "    masked_image = original_image * pred_mask\n",
    "\n",
    "    background_mask = np.concatenate([background_mask, background_mask, background_mask], axis=-1)\n",
    "    background_mask = background_mask * [0, 0, 0]\n",
    "\n",
    "    masked_image = masked_image + background_mask\n",
    "    name = path.split(\"/\")[-1]\n",
    "    cv2.imwrite(f\"RamBG Data/Outputs/{name}.png\", masked_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9e3ae5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b06d1dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77782e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6908ffc3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
